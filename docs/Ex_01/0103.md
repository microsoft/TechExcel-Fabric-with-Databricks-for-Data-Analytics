---
title: '3. Using data pipelines/data flow for data ingestion'
layout: default
nav_order: 3
parent: 'Exercise 1: Create a Fabric workspace and data pipeline'
---

# Task 1.3: Using data pipelines/data flow for data ingestion

There are multiple ways to ingest data into a Lakehouse, and in this exercise, Contoso focuses on using data pipelines and data flow to efficiently funnel diverse datasets into their system, setting the stage for advanced analytics and insights. 

1. In the lower left of the navigation pane for the workspace, select **Data Engineering** and then select **Data Factory**.

    ![selectDataFactory.jpg](../media/instructions254096/selectDataFactory.jpg)

1. Select the **Data pipeline** tile.

    ![dataFactory_pipeline.jpg](../media/instructions254096/dataFactory_pipeline.jpg)

1. Enter **Azure SQL DB Pipeline** for the pipeline name and then select **Create**.

1. Select the **Copy data assistant** tile.

    ![Copydataassistant.png](../media/instructions257645/Copydataassistant.png)
    
    <!-- !IMAGE[copyData.jpg](instructions254096/copyData.jpg) -->

1. On the Choose data source page, select **Azure SQL Database**. You may need to scroll down to see the Azure SQL Database option.

    ![selectSQLDatabase.jpg](../media/instructions254096/selectSQLDatabase.jpg)

1. Configure the connection by using the values in the following table. Leave all other settings at their default values.

    >{: .warning }
    >If there is no value listed for the Server setting, right-click the instructions pane in the lab environment and  select **Refresh**.

    | Default | Value |
    |:---------|:---------|
    | Server   | **@lab.Variable(sqlEndpoint)**   |
    | Database   | **Adventureworks**   |
    | Authentication kind   | **Basic**   |
    | Username   | **labsqladmin**   |
    | Password   | **Smoothie@2023**   |
    
    ![selectSQLDatabase2.jpg](../media/instructions254096/selectSQLDatabase2.jpg)


1. Select **Next**. Close any pop-up windows that display and wait for the connection to be created.

1. On the Connect to data source dialog, in the Select a table seciiont, select **Tables**.

1. Select **Select all**. Clear the **dbo.BuildVersion** and **dbo.ErrorLog** checkboxes and select **Next**.

    ![selecttables.jpg](../media/instructions254096/selecttables.jpg)

1. On the Choose data destinations page, search for and select +++Azure Data Lake Storage Gen2+++.

    ![adlsgen2.jpg](../media/instructions254096/adlsgen2.jpg)

1. On the Connect to data destination page, enter the following to create a new connection: 
    
    | Default | Value |
    |:---------|:---------|
    | URL   | **https://storage@lab.LabInstance.Id.dfs.core.windows.net/**   |
    | Authentication kind   | **Organizational account**   |
    

    <!-- | Tenant ID   | +++**@lab.Variable(serviceDirectoryID_tenantID)**+++   |
    | Service principal client ID   | +++**@lab.Variable(serviceApplicationID_clientID)**+++   |
    | Service principal Key   | +++**@lab.Variable(secretDescription)**+++   | -->

    >{: .important }
    >The connection URL for the Data Lake Storage account can be located here: Storage account > Settings > Endpoints > Data Lake Storage.
    >
    ![endpoint.jpg](../media/instructions254096/endpoint.jpg)   

1. Select **Sign in**.

1. Select the account that’s already authenticated and then select **Next**.

1. On the Connect to data destination page, next to the **Folder path** box, select **Browse**.

1. Select **medallion** > **bronze** and then select **OK**.

1. In the **File name suffix** box, enter **.csv** and then select **Next** to test the connection.

    ![selectbronzeandcsv.jpg](../media/instructions254096/selectbronzeandcsv.jpg)

1. Select **Next** and then Select **Save + Run**. After a brief delay, the Pipeline Run window displays.

    ![save+run.jpg](../media/instructions254096/save+run.jpg)

1. In the **Pipeline run** window, select **OK**. The pipeline will start processing.

1. On the upper-right of the page, select **Notifications**. You can use the Notifications area to monitor the pipeline.

    ![notifications.jpg](../media/instructions254096/notifications.jpg)

1. At the bottom left of the page, select **Data Factory**. The, in the Synapse section, select **Data Engineering**.

1. In the left navigation pane for the Synapse Data Engineering Home page, select **Monitor**.

    >{: .note }
    >You may need to select the ellipses (**...**) icon display the Monitor option.</br></br>The name for this page is in flux. You may see Monitor or you may see Monitoring Hub.
    
1. Verify that the value in the Status field for the pipeline is Succeeded.

    >{: .warning }
    >Please wait for the pipeline to execute. If the notification continues to say it’s running after 10 minutes, check the monitoring hub for a succeeded status.
    >![0faouzwm.png](../media/instructions249094/0faouzwm.png)

1. After the status shows **Succeeded**, your data has been transferred from Azure SQL Database to ADLS Gen 2.

    ![completedtransfer.jpg](../media/instructions254096/completedtransfer.jpg)

    >{: important }
    >Similarly, you can get data into the Lakehouses using pipelines from various other sources like Snowflake, Dataverse, and so on.
